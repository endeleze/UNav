# UNav Localizer Module

This folder provides the **modular, scalable localization pipeline** for the UNav navigation systemâ€”enabling robust visual localization across multi-building, multi-floor indoor environments.

All core steps are reusable Python APIs, with a unified pipeline and fine-grained control for both research and deployment.

---

## ðŸ“¦ Prerequisites

Please install **all dependencies** before using the localizer:

- **Mapping outputs:** Run the full UNav Mapping pipeline first to generate all required features, COLMAP models, and transform matrices.
- **CUDA GPU:** Strongly recommended for real-time performance.
- **Python libraries:**  
    - [implicit_dist](https://github.com/cvg/implicit_dist.git)
    - [PoseLib](https://github.com/vlarsson/PoseLib)
    - (For mapping) [stella_vslam_dense Docker](https://github.com/RoblabWh/stella_vslam_dense.git)
- Example installation:
    ```bash
    git clone https://github.com/cvg/implicit_dist.git
    cd implicit_dist
    pip install .

    git clone https://github.com/vlarsson/PoseLib.git
    cd PoseLib
    pip install .
    ```

---

## ðŸš€ Pipeline Overview

The UNav Localizer operates as follows:

1. **Feature Extraction:** Extract global & local features from the query image.
2. **VPR Retrieval:** Retrieve Top-K most similar database images across all registered places/floors.
3. **Data Preparation:** Load candidate map features and 3D geometry.
4. **Local Matching & RANSAC:** Batch geometric verification for all candidates, filtering outliers.
5. **Multi-frame Pose Refinement:** Optionalâ€”jointly refine pose with historical matches.
6. **Floorplan Projection:** Transform the refined 6DoF pose to 2D floorplan coordinates.

---

## ðŸš¦ Example Usage

```python
import cv2
from config import UNavConfig
from localizer.localizer import UNavLocalizer

# ---- 1. Build config and initialize ----
DATA_FINAL_ROOT = "/mnt/data/UNav-IO/data"
FEATURE_MODEL = "DinoV2Salad"
LOCAL_FEATURE_MODEL = "superpoint+lightglue"
PLACES = {
    "New_York_City": {
        "LightHouse": ["3_floor", "4_floor", "6_floor"]
    }
}

config = UNavConfig(
    data_final_root=DATA_FINAL_ROOT,
    places=PLACES,
    global_descriptor_model=FEATURE_MODEL,
    local_feature_model=LOCAL_FEATURE_MODEL
)
localizer_config = config.localizer_config

localizer = UNavLocalizer(localizer_config)
localizer.load_maps_and_features()

# ---- 2. Query image localization ----
img = cv2.imread("/mnt/data/UNav-IO/logs/New_York_City/LightHouse/6/02754/images/2023-07-18_09-40-46.png")

# Extract features
global_feat, local_feat_dict = localizer.extract_query_features(img)

# VPR retrieval
top_candidates = localizer.vpr_retrieve(global_feat, top_k=50)

# Candidate data preparation
candidates_data = localizer.get_candidates_data(top_candidates)

# Batch local matching + RANSAC
best_map_key, pnp_pairs, results = localizer.batch_local_matching_and_ransac(local_feat_dict, candidates_data)

# Multi-frame pose refinement (optional)
refinement_queue = {}
map_queue = refinement_queue.get(best_map_key, {"pairs": [], "initial_poses": [], "pps": []})
refine_result = localizer.multi_frame_pose_refine(pnp_pairs, img.shape, map_queue)

# Floorplan projection
colmap_pose = {"qvec": refine_result.get("qvec"), "tvec": refine_result.get("tvec")}
transform_matrix = localizer.transform_matrices.get(best_map_key, None)
floorplan_pose = (
    localizer.transform_pose_to_floorplan(colmap_pose["qvec"], colmap_pose["tvec"], transform_matrix)
    if (colmap_pose["tvec"] is not None and transform_matrix is not None)
    else None
)
```

---

## ðŸ§© Modular API Reference

Each pipeline stage is accessible as a standalone API:
```python
localizer.extract_query_features(query_img)
localizer.vpr_retrieve(global_feat, top_k)
localizer.get_candidates_data(top_candidates)
localizer.batch_local_matching_and_ransac(local_feat_dict, candidates_data)
localizer.multi_frame_pose_refine(pnp_pairs, img_shape, queue)
localizer.transform_pose_to_floorplan(qvec, tvec, transform_matrix)
```
Or use the **unified one-shot call**:
```python
result = localizer.localize(query_img, refinement_queue, top_k=...)
```

---

## ðŸ“‚ Directory Structure

```text
localizer/
â”‚
â”œâ”€â”€ localizer.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ io.py
â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ matcher.py
â”‚   â””â”€â”€ pnp.py
â”œâ”€â”€ visualization_tools/
â”‚   â””â”€â”€ localization_visualization_tools.py
â”œâ”€â”€ config.py
â””â”€â”€ README.md
```

---

## ðŸŽ¨ Visualization
- To simulate and visually debug the localization process, use the Jupyter notebook:

    ```
    /home/unav/Desktop/unav/visualization_localization.ipynb
    ```

  This notebook provides step-by-step demonstration and diagnostic tools for end-to-end visual localization evaluation.

---

## ðŸ’¡ Best Practices

- Call `load_maps_and_features()` before the first query, or after data changes.
- Use a **GPU** for all heavy computation steps (matching, RANSAC, refinement).
- For robust navigation, **multi-frame pose refinement** is recommended.
- The system supports seamless multi-building/multi-floor expansion with a single configuration.

---

## ðŸ‘¤ Contact

Developer: Anbang Yang (`ay1620@nyu.edu`)

_Last updated: 2025-05-27_
